{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usecase : Read the file movies realted document, decide whether clarification is needed. Summarise the text using the prompt to LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readme\n",
    "read_file_tool(state):\n",
    "Reads a text file from state[\"filepath\"] using UTF‑8.\n",
    "Returns {\"results\": file_text, \"confidence\": 0.95} on success; logs and returns empty results with 0.0 on error.\n",
    "Needs logger and Path imported.\n",
    "decide_if_clarify(state):\n",
    "Routes workflow: returns \"clarify\" if confidence < CONFIDENCE_THRESHOLD else \"summarize\".\n",
    "Needs CONFIDENCE_THRESHOLD and logger defined.\n",
    "ask_user_tool(state):\n",
    "Prompts the user for clarification using state[\"clarify_question\"].\n",
    "Returns {\"clarification\": \"...\"}.\n",
    "Raises if empty input. Uses logger.\n",
    "llm_summarize_tool(state):\n",
    "Sends state[\"results\"] to an LLM and returns {\"final_summary\": summary}.\n",
    "Runs the pipeline: read -> route -> optional clarify -> summarize -> save -> print final state.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key=\"YOUR_GROQ_API_KEY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x0000024957848D10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000024957849590>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(groq_api_key=groq_api_key,model=\"llama-3.1-8b-instant\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you're trying to initiate a conversation or test the connection. \n",
      "\n",
      "How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"Ping\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import Dict\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from typing import Dict, Optional\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "CONFIDENCE_THRESHOLD = 0.5          # threshold for “do we need clarification?”\n",
    "OUTPUT_FILE = \"summary_output.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_tool(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Reads the entire contents of a text file whose path is supplied in\n",
    "    ``state[\"filepath\"]`` and returns it as ``results`` together with a\n",
    "    high confidence score.\n",
    "\n",
    "    Expected input state keys:\n",
    "        - \"filepath\": str or pathlib.Path – absolute or relative path to the file.\n",
    "\n",
    "    Returns:\n",
    "        dict with keys:\n",
    "            - \"results\": str – the raw file contents (may be empty if the file is empty)\n",
    "            - \"confidence\": float – 0.95 if the file was read successfully,\n",
    "                                      0.0 otherwise (and an error is logged).\n",
    "    \"\"\"\n",
    "    raw_path = state.get(\"filepath\")\n",
    "    if not raw_path:\n",
    "        logger.error(\"No 'filepath' supplied in state.\")\n",
    "        return {\"results\": \"\", \"confidence\": 0.0}\n",
    "\n",
    "    try:\n",
    "        path = Path(raw_path).expanduser().resolve()\n",
    "        logger.info(\"Reading source file: %s\", path)\n",
    "\n",
    "        # Read the whole file as UTF‑8 text\n",
    "        content = path.read_text(encoding=\"utf-8\")\n",
    "        logger.info(\"[FILE_READ] %d characters read.\", len(content))\n",
    "\n",
    "        # We give a *high* confidence because the source is deterministic.\n",
    "        return {\"results\": content, \"confidence\": 0.95}\n",
    "    except Exception as exc:\n",
    "        logger.exception(\"Failed to read file %s: %s\", raw_path, exc)\n",
    "        return {\"results\": \"\", \"confidence\": 0.0}\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_if_clarify(state: dict) -> str:\n",
    "    \"\"\"\n",
    "    Simple router used by the workflow engine.\n",
    "    Returns the name of the next node:\n",
    "        - \"clarify\"   if confidence < CONFIDENCE_THRESHOLD\n",
    "        - \"summarize\" otherwise\n",
    "    \"\"\"\n",
    "    confidence = state.get(\"confidence\", 0.0)\n",
    "    logger.debug(\"Current confidence: %.2f\", confidence)\n",
    "    return \"clarify\" if confidence < CONFIDENCE_THRESHOLD else \"summarize\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_user_tool(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Prompts the user on the console for a clarification string.\n",
    "    The prompt can be customised via ``state[\"clarify_question\"]``.\n",
    "    \"\"\"\n",
    "    question = state.get(\"clarify_question\", \"Could you clarify your request?\")\n",
    "    answer = input(f\"\\nClarification needed: {question}\\nYour answer: \").strip()\n",
    "    if not answer:\n",
    "        raise ValueError(\"Clarification cannot be empty.\")\n",
    "    logger.info(\"[USER INPUT] %s\", answer)\n",
    "    return {\"clarification\": answer}\n",
    "\n",
    "def llm_summarize_tool(state: Dict[str, str]) -> Dict[str, Optional[str]]:\n",
    "    \"\"\"Summarise `state['results']` into five sentences.\"\"\"\n",
    "    raw_text = state.get(\"results\", \"\").strip()\n",
    "    if not raw_text:\n",
    "        print(f\"[SUMMARY] {summary}\\n\")\n",
    "        return {\"final_summary\": \"\"}\n",
    "\n",
    "    prompt =(\"You are a concise policy assistant. Use ONLY the provided text. \"\n",
    "        \"Summarize in 5 sentences and cite important terms.\\n\\n\" \n",
    "        f\"{raw_text}\")\n",
    "    # ← the result is a single AIMessage\n",
    "    result = llm.invoke([HumanMessage(content=prompt)])\n",
    "    summary = result.content\n",
    "    return {\"final_summary\": summary}\n",
    "\n",
    "def save_to_file_tool(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Writes ``state[\"final_summary\"]`` to ``OUTPUT_FILE`` (or a custom path\n",
    "    supplied via ``state[\"output_path\"]``) and returns the filename.\n",
    "    \"\"\"\n",
    "    filename = state.get(\"output_path\", OUTPUT_FILE)\n",
    "    try:\n",
    "        Path(filename).write_text(state.get(\"final_summary\", \"\"), encoding=\"utf-8\")\n",
    "        logger.info(\"[SAVED] %s\", filename)\n",
    "        return {\"file_saved\": filename}\n",
    "    except Exception as exc:\n",
    "        logger.exception(\"Failed to write output file %s: %s\", filename, exc)\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Reading source file: C:\\Users\\rkumbhar\\OneDrive - Deloitte (O365D)\\genai_project_assement_input.txt\n",
      "INFO:__main__:[FILE_READ] 774 characters read.\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:[SAVED] summary_output.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filepath': 'genai_project_assement_input.txt', 'results': 'Title: The Shawshank Redemption\\nGenre: Drama\\nRelease Year: 1994\\nRating: 9.3\\n\\nTitle: The Godfather\\nGenre: Crime, Drama\\nRelease Year: 1972\\nRating: 9.2\\n\\nTitle: The Dark Knight\\nGenre: Action, Crime, Drama\\nRelease Year: 2008\\nRating: 9.0\\n\\nTitle: Pulp Fiction\\nGenre: Crime, Drama\\nRelease Year: 1994\\nRating: 8.9\\n\\nTitle: Inception\\nGenre: Action, Sci-Fi, Thriller\\nRelease Year: 2010\\nRating: 8.8\\n\\nTitle: Forrest Gump\\nGenre: Drama, Romance\\nRelease Year: 1994\\nRating: 8.8\\n\\nTitle: Interstellar\\nGenre: Adventure, Drama, Sci-Fi\\nRelease Year: 2014\\nRating: 8.6\\n\\nTitle: The Matrix\\nGenre: Action, Sci-Fi\\nRelease Year: 1999\\nRating: 8.7\\n\\nTitle: Parasite\\nGenre: Comedy, Drama, Thriller\\nRelease Year: 2019\\nRating: 8.6\\n\\nTitle: Gladiator\\nGenre: Action, Adventure, Drama\\nRelease Year: 2000\\nRating: 8.5', 'confidence': 0.95, 'final_summary': \"The provided text lists top-rated films across various genres. Here's a concise summary:\\n\\nThe Shawshank Redemption (1994) has a 9.3 rating, making it the top-rated film. The Godfather (1972) follows closely with a 9.2 rating. The Dark Knight (2008) has a 9.0 rating. Pulp Fiction (1994), Inception (2010), Forrest Gump (1994), The Matrix (1999), Interstellar (2014), Parasite (2019), and Gladiator (2000) all have ratings between 8.5 and 8.9. Genre-wise, Drama is the most represented genre.\", 'file_saved': 'summary_output.txt'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the file\n",
    "    state = {\"filepath\": \"genai_project_assement_input.txt\"}          \n",
    "    state.update(read_file_tool(state))\n",
    "\n",
    "    #Decide next step\n",
    "    next_step = decide_if_clarify(state)\n",
    "\n",
    "    # If we need clarification, ask the user (otherwise skip)\n",
    "    if next_step == \"clarify\":\n",
    "        state.update(ask_user_tool(state))\n",
    "    # Summarise with the LLM\n",
    "    state.update(llm_summarize_tool(state))\n",
    "\n",
    "    # Save the result\n",
    "    state.update(save_to_file_tool(state))\n",
    "\n",
    "    # Print final state for debugging (optional)\n",
    "    print(state)\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
